{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code reads in the original AFDC stations dataset, assigns a unique ID and BG-level FIPS codes, and drops irrelevant columns and stations (e.g. out of territory of interest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "import strgen\n",
    "from tqdm import tqdm\n",
    "import state_name_crs_mappings_ML as crsm\n",
    "import us\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant paths\n",
    "root = ''\n",
    "path = root + 'Data/'\n",
    "path_US_data = root + 'Data/geodata/'\n",
    "result_path = root + 'final_data/'\n",
    "path_IRA = root + 'Data/IRA/1.0-shapefile-codebook/usa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in original station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "df_stations = pd.read_csv(path + 'AFDC/alt_fuel_stations (Apr 3 2023).csv')\n",
    "print(len(df_stations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign UID based on original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create unique ID\n",
    "\n",
    "# Street part\n",
    "df_stations['temp'] = df_stations['Street Address'].str.replace(' ','').str[:5]\n",
    "# For stations without street address, create a random string\n",
    "for ind in tqdm(df_stations.loc[df_stations['temp'].isna()].index.to_list()):\n",
    "    df_stations.loc[ind,'temp'] = strgen.StringGenerator(\"[\\w\\d]{5}\").render()\n",
    "\n",
    "# Random part\n",
    "df_stations['temp2'] = ''\n",
    "for ind in tqdm(df_stations.index):\n",
    "    df_stations.loc[ind,'temp2'] = strgen.StringGenerator(\"[\\w\\d]{5}\").render()\n",
    "\n",
    "# Combine\n",
    "df_stations['unique_ID'] = df_stations['State'] + df_stations['ZIP'].astype(str) + df_stations['temp'] + df_stations['temp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UID is nan if no state or ZIP is provided - fill these with unique IDs\n",
    "index_UID_NaN = df_stations.loc[df_stations['unique_ID'].isna()].index\n",
    "df_stations.loc[index_UID_NaN, 'unique_ID'] = 'US' + df_stations.loc[index_UID_NaN]['ZIP'].astype(str) + df_stations.loc[index_UID_NaN]['temp'] + df_stations.loc[index_UID_NaN]['temp2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the data\n",
    "df_stations.drop('temp',axis=1,inplace=True)\n",
    "df_stations.drop('temp2',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index\n",
    "df_stations.set_index('unique_ID',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data\n",
    "df_stations.to_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wUID.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop irrelevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "df_stations = pd.read_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wUID.csv',index_col=0)\n",
    "df_stations.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop some columns which are certaintly not needed because they are empty or not relevant for EV charging\n",
    "df_stations.drop(columns='Plus4',inplace=True)\n",
    "df_stations.drop(columns='BD Blends',inplace=True)\n",
    "df_stations.drop(columns='NG Fill Type Code',inplace=True)\n",
    "df_stations.drop(columns='NG PSI',inplace=True)\n",
    "df_stations.drop(columns='Hydrogen Status Link',inplace=True)\n",
    "df_stations.drop(columns='NG Vehicle Class',inplace=True)\n",
    "df_stations.drop(columns='LPG Primary',inplace=True)\n",
    "df_stations.drop(columns='E85 Blender Pump',inplace=True)\n",
    "df_stations.drop(columns='Intersection Directions (French)',inplace=True)\n",
    "df_stations.drop(columns='Access Days Time (French)',inplace=True)\n",
    "df_stations.drop(columns='BD Blends (French)',inplace=True)\n",
    "df_stations.drop(columns='Groups With Access Code (French)',inplace=True)\n",
    "df_stations.drop(columns='Hydrogen Is Retail',inplace=True)\n",
    "df_stations.drop(columns='CNG Dispenser Num',inplace=True)\n",
    "df_stations.drop(columns='CNG On-Site Renewable Source',inplace=True)\n",
    "df_stations.drop(columns='CNG Total Compression Capacity',inplace=True)\n",
    "df_stations.drop(columns='CNG Storage Capacity',inplace=True)\n",
    "df_stations.drop(columns='LNG On-Site Renewable Source',inplace=True)\n",
    "df_stations.drop(columns='E85 Other Ethanol Blends',inplace=True)\n",
    "df_stations.drop(columns='EV Pricing (French)',inplace=True)\n",
    "df_stations.drop(columns='LPG Nozzle Types',inplace=True)\n",
    "df_stations.drop(columns='Hydrogen Pressures',inplace=True)\n",
    "df_stations.drop(columns='Hydrogen Standards',inplace=True)\n",
    "df_stations.drop(columns='CNG Fill Type Code',inplace=True)\n",
    "df_stations.drop(columns='CNG PSI',inplace=True)\n",
    "df_stations.drop(columns='CNG Vehicle Class',inplace=True)\n",
    "df_stations.drop(columns='LNG Vehicle Class',inplace=True)\n",
    "df_stations.drop(columns='RD Blends',inplace=True)\n",
    "df_stations.drop(columns='RD Blends (French)',inplace=True)\n",
    "df_stations.drop(columns='RD Blended with Biodiesel',inplace=True)\n",
    "df_stations.drop(columns='RD Maximum Biodiesel Level',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign FIPS information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to geodata\n",
    "gdf_stations = geopd.GeoDataFrame(df_stations, geometry=geopd.points_from_xy(df_stations.Longitude, df_stations.Latitude, crs=\"EPSG:4326\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State info (actually given but some are missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read state shape file and convert to stations' crs\n",
    "gdf_states = geopd.read_file(path_US_data + 'tl_2017_us_state/tl_2017_us_state.shp')\n",
    "gdf_states = gdf_states.to_crs(gdf_stations.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use state shapefile to identify state by geometry to check for mistakes\n",
    "# New state column assigned by geometry: STUSPS\n",
    "gdf_stations_wstates = gdf_stations.sjoin(gdf_states[['STUSPS','geometry']], how='left', predicate='within')\n",
    "gdf_stations_wstates.drop(columns='index_right',inplace=True)\n",
    "gdf_stations_wstates.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state FIPS code (str)\n",
    "gdf_stations_wstates['STATEFP'] = ''\n",
    "for state in gdf_stations['State'].unique():\n",
    "    try:\n",
    "        gdf_stations_wstates.loc[gdf_stations_wstates['State']==state,'STATEFP'] = us.states.lookup(state).fips\n",
    "    except:\n",
    "        # Handle missing or invalid state names\n",
    "        print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually assign DC FIPS code\n",
    "gdf_stations_wstates.loc[gdf_stations_wstates['State'] == 'DC','STATEFP'] = '11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct stations with QC state code but located in CA\n",
    "gdf_stations_wstates.loc['QC913165566Y8XvpW','State'] = 'CA'\n",
    "gdf_stations_wstates.loc['QC913165566Y8XvpW','STATEFP'] = '06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ON -- Canadian, remove\n",
    "gdf_stations_wstates = gdf_stations_wstates.loc[gdf_stations_wstates['State'] != 'ON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for KA -- Wrongly listed, remove\n",
    "gdf_stations_wstates = gdf_stations_wstates.loc[gdf_stations_wstates['State'] != 'KA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could some not be assigned? --> Remove them/wrong geolocation\n",
    "print(len(gdf_stations_wstates.loc[gdf_stations_wstates['STUSPS'].isna()]))\n",
    "gdf_stations_wstates = gdf_stations_wstates.loc[~gdf_stations_wstates['STUSPS'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stations where state by attribute does not match state by geometry\n",
    "print(len(gdf_stations_wstates.loc[gdf_stations_wstates['State'] != gdf_stations_wstates['STUSPS']]))\n",
    "gdf_stations_wstates = gdf_stations_wstates.loc[gdf_stations_wstates['State'] == gdf_stations_wstates['STUSPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remaining stations all have State == STUSPS--> Drop STUSPS column\n",
    "assert (gdf_stations_wstates['State'] == gdf_stations_wstates['STUSPS']).all()\n",
    "gdf_stations_wstates.drop(columns='STUSPS',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove PR\n",
    "gdf_stations_wstates = gdf_stations_wstates.loc[gdf_stations_wstates['State'] != 'PR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "# Commented to keep original file\n",
    "df_stations_wstates = gdf_stations_wstates.drop(columns='geometry')\n",
    "df_stations_wstates.to_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read county shape file\n",
    "gdf_county = geopd.read_file(path_US_data + 'tl_2022_us_county/tl_2022_us_county.shp')\n",
    "gdf_county = gdf_county.to_crs(gdf_stations.crs)\n",
    "gdf_county.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use state shapefile to identify county by geometry\n",
    "gdf_stations_wcounties = gdf_stations_wstates.sjoin(gdf_county[['GEOID','geometry']], how='left', predicate='within')\n",
    "gdf_stations_wcounties.drop(columns='index_right',inplace=True)\n",
    "gdf_stations_wcounties.rename(columns={'GEOID':'COUNTYFP'},inplace=True)\n",
    "gdf_stations_wcounties.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_stations_wcounties = gdf_stations_wcounties.drop(columns='geometry')\n",
    "df_stations_wcounties.to_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign state name - required to read IRA files later\n",
    "gdf_stations_wcounties['State_Name'] = None\n",
    "for state_fips in gdf_stations_wcounties['STATEFP'].unique():\n",
    "    if state_fips == '11':\n",
    "        state_name = 'District of Columbia'\n",
    "    else:\n",
    "        state_name = us.states.lookup(state_fips).name\n",
    "    gdf_stations_wcounties.loc[gdf_stations_wcounties['STATEFP'] == state_fips,'State_Name'] = state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign tracts\n",
    "gdf_stations_wtracts = pd.DataFrame()\n",
    "for state in tqdm(gdf_stations_wcounties['STATEFP'].unique()):\n",
    "    # Filter stations in state\n",
    "    gdf = gdf_stations_wcounties.loc[gdf_stations_wcounties['STATEFP'] == state]\n",
    "    state_name = gdf['State_Name'].iloc[0]\n",
    "    # Read IRA file\n",
    "    gdf_IRA = geopd.read_file(path_IRA + state_name.replace(' ','') + '.shp')\n",
    "    gdf_IRA = gdf_IRA.to_crs(gdf_stations.crs)\n",
    "    # Assign tracts\n",
    "    gdf = gdf.sjoin(gdf_IRA[['GEOID10','geometry']], how='left', predicate='within')\n",
    "    gdf.drop(columns='index_right',inplace=True)\n",
    "    # Append\n",
    "    if len(gdf_stations_wtracts) > 0:\n",
    "        gdf_stations_wtracts = pd.concat([gdf_stations_wtracts,gdf])\n",
    "    else:\n",
    "        gdf_stations_wtracts = gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "gdf_stations_wtracts.rename(columns={'GEOID10':'TRACTFP'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_stations_wtracts = gdf_stations_wtracts.drop(columns='geometry')\n",
    "df_stations_wtracts.to_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign BGs\n",
    "gdf_stations_wbgs = pd.DataFrame()\n",
    "for fips in tqdm(gdf_stations_wtracts['STATEFP'].unique()):\n",
    "    # Filter stations in state\n",
    "    gdf = gdf_stations_wtracts.loc[gdf_stations_wtracts['STATEFP'] == fips]\n",
    "    # Read BG file\n",
    "    file_bg = path + 'geodata/tl_bg/tl_2020_'+fips+'_bg/tl_2020_'+fips+'_bg.shp'\n",
    "    gdf_bg = geopd.read_file(file_bg)\n",
    "    gdf_bg = gdf_bg.to_crs(gdf_stations_wtracts.crs)\n",
    "    # Assign tracts\n",
    "    gdf = gdf.sjoin(gdf_bg[['GEOID','ALAND','geometry']], how='left', predicate='within')\n",
    "    gdf.drop(columns='index_right',inplace=True)\n",
    "    # Append\n",
    "    if len(gdf_stations_wbgs) > 0:\n",
    "        gdf_stations_wbgs = pd.concat([gdf_stations_wbgs,gdf])\n",
    "    else:\n",
    "        gdf_stations_wbgs = gdf.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "gdf_stations_wbgs.rename(columns={'GEOID':'BGFP'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save shapefile\n",
    "gdf_stations_wbgs.to_file(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_stations_wbgs = gdf_stations_wbgs.drop(columns='geometry')\n",
    "df_stations_wbgs.to_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
