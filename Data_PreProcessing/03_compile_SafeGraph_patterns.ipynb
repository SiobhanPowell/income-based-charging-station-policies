{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc2126f7",
   "metadata": {},
   "source": [
    "# Compile patterns data\n",
    "The original POI pattern data (popularity by hour, etc.) is split across 30 large files. Here we load and combine the relevant data into one file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2e95e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ''\n",
    "folder = root + 'Data/SafeGraphData/DownloadedData/2022/10/PG/'\n",
    "upper_folder = root + 'Data/SafeGraphData/DownloadedData/2022/10/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c49f4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34310567",
   "metadata": {},
   "source": [
    "### Load compiled file without patterns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ff8ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(folder+'compiled.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782074f7",
   "metadata": {},
   "source": [
    "### Compile patterns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69f046aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pattern_data(tmp2):\n",
    "    tmp3 = tmp2.loc[:, ['placekey', 'raw_visit_counts', 'raw_visitor_counts', 'distance_from_home', 'median_dwell', 'normalized_visits_by_state_scaling']].copy(deep=True)\n",
    "    tmp4 = pd.DataFrame(tmp2[\"popularity_by_day\"].str.split('{|,|:|}', expand=True).values, columns=np.arange(0, 16)).loc[:, [2, 4, 6, 8, 10, 12, 14]]\n",
    "    tmp3.loc[:, ['pop_by_day_'+i for i in ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']]] = tmp4.values\n",
    "    dwell_buckets = [\"<5\", \"5-10\", \"11-20\", \"21-60\", \"61-120\", \"121-240\", \">240\"]\n",
    "    tmp3.loc[:, ['dwell_time_bucket_'+i for i in dwell_buckets]] = pd.DataFrame(tmp2[\"bucketed_dwell_times\"].str.split('{|:|,|}', expand=True).values, columns=np.arange(16)).loc[:, [2,4,6,8,10,12,14]].values\n",
    "    tmp3.loc[:, ['related_same_day_brand_top'+str(i) for i in [1,2,3]]] = tmp2['related_same_day_brand'].str.split('{|,|:|}', expand=True).loc[:, [1, 3, 5]].values\n",
    "    tmp5 = tmp2['popularity_by_hour'].str.replace('[', '').str.replace(']', '').str.split(',', expand=True)\n",
    "    inds = tmp5[tmp5.isna().sum(axis=1)==0].index\n",
    "    hour_names = ['0to3', '3to6', '6to9', '9to12', '12to15', '15to18', '18to21', '21to24']\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[0]] = tmp5.loc[inds, 0:2].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[1]] = tmp5.loc[inds, 3:5].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[2]] = tmp5.loc[inds, 6:8].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[3]] = tmp5.loc[inds, 9:11].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[4]] = tmp5.loc[inds, 12:14].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[5]] = tmp5.loc[inds, 15:17].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[6]] = tmp5.loc[inds, 18:20].astype(int).sum(axis=1).values\n",
    "    tmp3.loc[inds, 'popularity_by_hour_'+hour_names[7]] = tmp5.loc[inds, 21:23].astype(int).sum(axis=1).values\n",
    "\n",
    "    for col in ['raw_visit_counts', 'raw_visitor_counts', 'distance_from_home',\n",
    "           'median_dwell', 'normalized_visits_by_state_scaling',\n",
    "           'pop_by_day_Monday', 'pop_by_day_Tuesday', 'pop_by_day_Wednesday',\n",
    "           'pop_by_day_Thursday', 'pop_by_day_Friday', 'pop_by_day_Saturday',\n",
    "           'pop_by_day_Sunday', 'dwell_time_bucket_<5', 'dwell_time_bucket_5-10',\n",
    "           'dwell_time_bucket_11-20', 'dwell_time_bucket_21-60',\n",
    "           'dwell_time_bucket_61-120', 'dwell_time_bucket_121-240',\n",
    "           'dwell_time_bucket_>240', \n",
    "           'popularity_by_hour_0to3', 'popularity_by_hour_3to6',\n",
    "           'popularity_by_hour_6to9', 'popularity_by_hour_9to12',\n",
    "           'popularity_by_hour_12to15', 'popularity_by_hour_15to18',\n",
    "           'popularity_by_hour_18to21', 'popularity_by_hour_21to24']:\n",
    "        inds = tmp3[~tmp3[col].isna()].index\n",
    "        tmp3.loc[inds, col] = tmp3.loc[inds, col].astype(int)\n",
    "\n",
    "    return tmp3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4b38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = None\n",
    "\n",
    "for file_number in np.arange(1, 30):\n",
    "#     print('File number: ', file_number)\n",
    "    file = 'core_poi-geometry-patterns-part'+str(file_number)+'.csv'\n",
    "    tmp = pd.read_csv(folder+file)\n",
    "    tmp2 = tmp.loc[tmp['iso_country_code']=='US']\n",
    "    tmp4 = process_pattern_data(tmp2)\n",
    "    if patterns is None:\n",
    "        patterns = tmp4.copy(deep=True).reset_index(drop=True)\n",
    "    else:\n",
    "        patterns = pd.concat((patterns, tmp4.copy(deep=True).reset_index(drop=True)), axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2c9c1c",
   "metadata": {},
   "source": [
    "### Save patterns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bef11e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns.to_csv(folder+'compiled_patterns.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde0259e",
   "metadata": {},
   "source": [
    "### Combine two compiled dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b09a6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df.merge(patterns, how='outer', on='placekey')\n",
    "df_full.to_csv(upper_folder+'compiled_plus_patterns.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
