{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file compiles the main station-level dataset used for the clustering and the definition of the station topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import us\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import category_groupings_250403 as cg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = ''\n",
    "path = root + 'Data/'\n",
    "path_US_data = root + 'Data/geodata/'\n",
    "result_path = root + 'final_data/'\n",
    "path_IRA = root + 'Data/IRA/1.0-shapefile-codebook/usa/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation state-specific station-level datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which states to compile\n",
    "states = []\n",
    "for state in us.states.STATES:\n",
    "    states +=[state.abbr]\n",
    "states += ['DC']\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "label = '250415'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original stations wFIPS\n",
    "df_stations = pd.read_csv(result_path + '00_alt_fuel_stations (Apr 3 2023)_wFIPS.csv',index_col=0) # ,dtype={'STATEFP':int,'COUNTYFP':int,'BGFP':int})\n",
    "df_stations['STATEFP'] = df_stations['STATEFP'].astype(str).str.zfill(2)\n",
    "df_stations['COUNTYFP'] = df_stations['COUNTYFP'].astype(str).str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NaN in BGFP and convert to str\n",
    "df_stations = df_stations.loc[~df_stations['BGFP'].isna()]\n",
    "df_stations['BGFP'] = df_stations['BGFP'].astype(int).astype(str).str.zfill(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for relevant states\n",
    "df_stations = df_stations[df_stations.State.isin(states)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add BG socioeconomics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect\n",
    "df_socioecon = pd.read_csv(result_path + 'BGlevel/level_BG.csv')\n",
    "df_socioecon['STATEFP'] = df_socioecon['STATEFP'].astype(str).str.zfill(2)\n",
    "df_socioecon['COUNTYFP'] = df_socioecon['COUNTYFP'].astype(str).str.zfill(5)\n",
    "df_socioecon['BGFP'] = df_socioecon['BGFP'].astype(str).str.zfill(12)\n",
    "df_socioecon.set_index('BGFP', inplace=True)\n",
    "df_socioecon.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add socioeconomic data\n",
    "for state in (states):\n",
    "    print(state)\n",
    "    if state == 'DC':\n",
    "        fips = '11'\n",
    "    else:\n",
    "        fips = us.states.lookup(state).fips\n",
    "    \n",
    "    # Filter for stations in state\n",
    "    df_stations_state = df_stations.loc[df_stations['STATEFP'] == fips]\n",
    "    df_stations_state.reset_index(inplace=True)\n",
    "\n",
    "    # Filter for socioeocnomic data in state\n",
    "    df_socioecon_state = df_socioecon.loc[df_socioecon['STATEFP'] == fips]\n",
    "\n",
    "    # Drop one of the COUNTYFP columns\n",
    "    df_socioecon_state = df_socioecon_state.drop(['COUNTYFP'],axis=1,inplace=False)\n",
    "\n",
    "    # Merge stations and income\n",
    "    df_stations_state = pd.merge(df_stations_state, df_socioecon_state, how='left', on=['BGFP','STATEFP']) #,right_index=True)\n",
    "    df_stations_state.set_index('unique_ID', inplace=True)\n",
    "\n",
    "    # Save\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add nb income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column duplicates\n",
    "cols_dup = ['median_household_income_byBG','total_pop_byBG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nb income\n",
    "df_stations_US = pd.DataFrame()\n",
    "for state in tqdm(states):\n",
    "    if state == 'DC':\n",
    "        fips = '11'\n",
    "    else:\n",
    "        fips = us.states.lookup(state).fips\n",
    "    # Read current station file\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['STATEFP'] = df_stations_state['STATEFP'].astype(str).str.zfill(2)\n",
    "    df_stations_state['COUNTYFP'] = df_stations_state['COUNTYFP'].astype(str).str.zfill(5)\n",
    "    df_stations_state['BGFP'] = df_stations_state['BGFP'].astype(int).astype(str).str.zfill(12)\n",
    "    # Read nb income\n",
    "    df_nbincome_state = pd.read_csv(result_path + 'BGlevel/level_BG_'+state+'_wnbincome.csv')\n",
    "    df_nbincome_state['STATEFP'] = df_nbincome_state['STATEFP'].astype(str).str.zfill(2)\n",
    "    df_nbincome_state['COUNTYFP'] = df_nbincome_state['COUNTYFP'].astype(str).str.zfill(5)\n",
    "    df_nbincome_state['BGFP'] = df_nbincome_state['BGFP'].astype(str).str.zfill(12)\n",
    "    # Drop duplicated columns\n",
    "    for col_del in cols_dup:\n",
    "        df_nbincome_state.drop(col_del,axis=1,inplace=True)\n",
    "    # Drop one of the COUNTYFP columns\n",
    "    df_nbincome_state.drop('COUNTYFP',axis=1,inplace=True)\n",
    "    # Merge   \n",
    "    df_stations_state['unique_ID'] = df_stations_state.index \n",
    "    df_stations_state = df_stations_state.merge(df_nbincome_state, how='left', on=['BGFP','STATEFP'])\n",
    "    df_stations_state.set_index('unique_ID', inplace=True)\n",
    "    \n",
    "    # Save data\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add PoI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PoIs within which distance\n",
    "distance = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all meta categories used\n",
    "list_top_cats = []\n",
    "for key in cg.category_grouping('a',return_key_list=True):\n",
    "    list_top_cats += [cg.category_grouping(key)]\n",
    "list_top_cats = sorted(set(list_top_cats))\n",
    "list_top_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include new columns: no of POI and share of each POI type\n",
    "df_stations_US['no_PoI_'+str(distance)] = 0\n",
    "for top_cat in list_top_cats:\n",
    "    df_stations_US['no_'+top_cat.replace(' ','_')+'_'+str(distance)] = 0\n",
    "    df_stations_US['share_'+top_cat.replace(' ','_')+'_'+str(distance)] = 0.0    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PoI number and shares\n",
    "for state in (states):\n",
    "    print(state)\n",
    "    # Read station data\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['STATEFP'] = df_stations_state['STATEFP'].astype(str).str.zfill(2)\n",
    "    df_stations_state['COUNTYFP'] = df_stations_state['COUNTYFP'].astype(str).str.zfill(5)\n",
    "    df_stations_state['BGFP'] = df_stations_state['BGFP'].astype(int).astype(str).str.zfill(12)\n",
    "    # Read PoI\n",
    "    df_dewey_state = pd.read_csv(result_path + 'Dewey/04b_compiled_'+state+'_addinfo.csv',index_col=0)\n",
    "    df_dewey_state.set_index('placekey',inplace=True)\n",
    "    # Iterate over counties\n",
    "    for countyfp in tqdm(df_stations_state['COUNTYFP'].unique()):\n",
    "        # Filter for stations\n",
    "        df_stations_county = df_stations_state.loc[df_stations_state['COUNTYFP'] == countyfp]\n",
    "        # Read distance matrix\n",
    "        df_distance_matrix_county = pd.read_csv(result_path + 'distancematrices_uniqueID/'+state+'_'+countyfp + '_distancematrix.csv',index_col=0)\n",
    "        # Iterate over stations in county\n",
    "        for unique_ID in df_stations_county.index:\n",
    "            # Filter for PoI within range\n",
    "            df = df_distance_matrix_county.loc[df_distance_matrix_county[unique_ID] <= distance]\n",
    "            df_stations_state.loc[unique_ID,'no_PoI_500'] = len(df)\n",
    "            # If there is at least one PoI, add number of PoI by category\n",
    "            if len(df) > 0:\n",
    "                df2 = df_dewey_state.loc[df.index]\n",
    "                for top_cat in df2.top_category_edit.unique():\n",
    "                    if str(top_cat) != 'nan':\n",
    "                        df3 = df2.loc[df2['top_category_edit'] == top_cat]\n",
    "                        df_stations_state.loc[unique_ID,'no_' + top_cat.replace(' ','_') + '_500'] = len(df3)\n",
    "                        df_stations_state.loc[unique_ID,'share_' + top_cat.replace(' ','_') + '_500'] = len(df3)/len(df)\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read patterns data\n",
    "df_patterns = pd.read_csv(result_path + 'patternsonly_uniqueID_20250401.csv',index_col=0)\n",
    "df_patterns.set_index('unique_ID', inplace=True)\n",
    "df_patterns.drop('State',axis=1,inplace=True)\n",
    "df_patterns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge patterns and stations\n",
    "for state in states:\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['BGFP'] = df_stations_state['BGFP'].astype(int).astype(str).str.zfill(12)\n",
    "    df_stations_state = df_stations_state.merge(df_patterns, how='left', left_index=True, right_index=True)\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add county-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read counry-level data\n",
    "gdf_county = pd.read_csv(result_path + 'level_county.csv',index_col=0)\n",
    "gdf_county['COUNTYFP'] = gdf_county['COUNTYFP'].astype(str).str.zfill(5)\n",
    "gdf_county.drop('STATEFP', axis=1, inplace=True) # to avoid double columns\n",
    "gdf_county.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge county data and stations\n",
    "for state in states:\n",
    "    # Read stations\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['COUNTYFP'] = df_stations_state['COUNTYFP'].astype(int).astype(str).str.zfill(5)\n",
    "    df_stations_state['unique_ID'] = df_stations_state.index\n",
    "    # Merge\n",
    "    df_stations_state = df_stations_state.merge(gdf_county, how='left', left_on='COUNTYFP', right_on='COUNTYFP')\n",
    "    df_stations_state.set_index('unique_ID', inplace=True)\n",
    "    # Save\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add state-level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add state-level info\n",
    "df_state_level = pd.read_csv(result_path + 'level_state.csv')\n",
    "df_state_level['STATEFP'] = df_state_level['STATEFP'].astype(str).str.zfill(2)\n",
    "df_state_level.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge county data and stations\n",
    "for state in states:\n",
    "    # Read stations\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['STATEFP'] = df_stations_state['STATEFP'].astype(int).astype(str).str.zfill(2)\n",
    "    df_stations_state['unique_ID'] = df_stations_state.index\n",
    "    # Merge\n",
    "    df_stations_state = df_stations_state.merge(df_state_level, how='left', on='STATEFP')\n",
    "    df_stations_state.set_index('unique_ID', inplace=True)\n",
    "    # Save\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add highways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge hwys and stations\n",
    "for state in states:\n",
    "    # Read stations\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['BGFP'] = df_stations_state['BGFP'].astype(int).astype(str).str.zfill(12)\n",
    "    # Read hwy distance\n",
    "    gdf_hwy_state = pd.read_csv(result_path + 'stationlevel/level_stations_'+state+'_hwy.csv',index_col=0)\n",
    "    # Merge\n",
    "    df_stations_state = df_stations_state.merge(gdf_hwy_state, how='left', left_index=True, right_index=True)\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add number of stations nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add number of stations nearby\n",
    "for state in (states):\n",
    "    print(state)\n",
    "    # Read station data\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    df_stations_state['BGFP'] = df_stations_state['BGFP'].astype(int).astype(str).str.zfill(12)\n",
    "    df_stations_state['nostations_nearby_'+str(distance)] = 0\n",
    "    # Read distance matrix\n",
    "    df_distance_matrix = pd.read_csv(result_path + 'distancematrices_stations_uniqueID/'+state+'_stations_distancematrix.csv',index_col=0)\n",
    "    # Iterate over stations in county\n",
    "    for unique_ID in tqdm(df_stations_state.index):\n",
    "        # Filter for PoI within range\n",
    "        df = df_distance_matrix.loc[df_distance_matrix[unique_ID] <= distance]\n",
    "        df_stations_state.loc[unique_ID,'nostations_nearby_'+str(distance)] = len(df) - 1 # minus own station\n",
    "    df_stations_state.to_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine state-specific datasets for entire US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All states\n",
    "states = []\n",
    "for state in us.states.STATES:\n",
    "    states +=[state.abbr]\n",
    "states += ['DC']\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piece state-level datasets together\n",
    "df_stations_US = pd.DataFrame()\n",
    "for state in states:\n",
    "    # Read data\n",
    "    df_stations_state = pd.read_csv(result_path + 'stationlevel/20_level_stations_' + state + '_compiled.csv',index_col=0)\n",
    "    # Compile\n",
    "    if state == states[0]:\n",
    "        df_stations_US = df_stations_state.copy()\n",
    "    else:\n",
    "        df_stations_US = pd.concat([df_stations_US, df_stations_state], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename\n",
    "df_stations_US.rename(columns={'ResCars_pp_BG_byBG':'ResCars_pp_BG'}, inplace=True)\n",
    "df_stations_US.rename(columns={'PopDensity_byBG':'PopDensity_inBG'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "df_stations_US.to_csv(result_path + 'stationlevel/20_level_stations_US_compiled_'+label+'.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
